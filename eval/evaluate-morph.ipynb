{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140bf0b2-d1f4-4a64-85d0-f9eef8729ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import IPython\n",
    "from IPython.display import Audio, display\n",
    "import torch\n",
    "import librosa, librosa.display\n",
    "from tqdm import tqdm\n",
    "from stqdm import stqdm\n",
    "import json\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "from audioldm2 import text_to_audio, build_model, seed_everything, make_batch_for_text_to_audio\n",
    "from audioldm2.latent_diffusion.modules.diffusionmodules.util import (\n",
    "    make_ddim_sampling_parameters,\n",
    "    make_ddim_timesteps,\n",
    "    noise_like,\n",
    "    extract_into_tensor,\n",
    ")\n",
    "from audioldm2.latent_diffusion.models.ddim import DDIMSampler\n",
    "from audioldm2.utilities import *\n",
    "from audioldm2.utilities.audio import *\n",
    "from audioldm2.utilities.data import *\n",
    "from audioldm2.utils import default_audioldm_config\n",
    "\n",
    "from audioldm2.gaverutils import gaver_sounds\n",
    "\n",
    "from audioldm2.latent_diffusion.modules.attention import SpatialTransformer, CrossAttention\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "from interfaces.diffusion_helper_qkv import *\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404b660f-b846-4543-a5ad-e5fac8bcb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb8c1e8-e999-44e9-b554-d8d922f87baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5ff339-5dc0-4050-ab09-90201f7c2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    print('Loading model')\n",
    "    \n",
    "    latent_diffusion = build_model(model_name=model_name)\n",
    "    latent_diffusion.latent_t_size = int(duration * latent_t_per_second)\n",
    "\n",
    "    print('Model loaded')\n",
    "    return latent_diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169ed5cd-7b84-4b03-b547-0b31dca347d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading AudioLDM-2: audioldm_16k_crossattn_t5\n",
      "Loading model on cuda:0\n",
      "{'variables': {'sampling_rate': 16000, 'mel_bins': 64, 'latent_embed_dim': 8, 'latent_t_size': 256, 'latent_f_size': 16, 'in_channels': 8, 'optimize_ddpm_parameter': True, 'warmup_steps': 5000}, 'step': {'validation_every_n_epochs': 1, 'save_checkpoint_every_n_steps': 5000, 'max_steps': 1500000, 'save_top_k': 2}, 'preprocessing': {'audio': {'sampling_rate': 16000, 'max_wav_value': 32768, 'duration': 10.24}, 'stft': {'filter_length': 1024, 'hop_length': 160, 'win_length': 1024}, 'mel': {'n_mel_channels': 64, 'mel_fmin': 0, 'mel_fmax': 8000}}, 'augmentation': {'mixup': 0}, 'model': {'target': 'audioldm2.latent_diffusion.models.ddpm.LatentDiffusion', 'params': {'first_stage_config': {'base_learning_rate': 8e-06, 'target': 'audioldm2.latent_encoder.autoencoder.AutoencoderKL', 'params': {'sampling_rate': 16000, 'batchsize': 4, 'monitor': 'val/rec_loss', 'image_key': 'fbank', 'subband': 1, 'embed_dim': 8, 'time_shuffle': 1, 'lossconfig': {'target': 'audioldm2.latent_diffusion.modules.losses.LPIPSWithDiscriminator', 'params': {'disc_start': 50001, 'kl_weight': 1000, 'disc_weight': 0.5, 'disc_in_channels': 1}}, 'ddconfig': {'double_z': True, 'mel_bins': 64, 'z_channels': 8, 'resolution': 256, 'downsample_time': False, 'in_channels': 1, 'out_ch': 1, 'ch': 128, 'ch_mult': [1, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0}}}, 'base_learning_rate': 0.0001, 'warmup_steps': 5000, 'optimize_ddpm_parameter': True, 'sampling_rate': 16000, 'batchsize': 16, 'linear_start': 0.0015, 'linear_end': 0.0195, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'unconditional_prob_cfg': 0.1, 'parameterization': 'eps', 'first_stage_key': 'fbank', 'latent_t_size': 256, 'latent_f_size': 16, 'channels': 8, 'monitor': 'val/loss_simple_ema', 'scale_by_std': True, 'unet_config': {'target': 'audioldm2.latent_diffusion.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 64, 'context_dim': [1024], 'in_channels': 8, 'out_channels': 8, 'model_channels': 128, 'attention_resolutions': [8, 4, 2], 'num_res_blocks': 2, 'channel_mult': [1, 2, 3, 5], 'num_head_channels': 32, 'use_spatial_transformer': True, 'transformer_depth': 1}}, 'evaluation_params': {'unconditional_guidance_scale': 3.5, 'ddim_sampling_steps': 200, 'n_candidates_per_samples': 3}, 'cond_stage_config': {'crossattn_flan_t5': {'cond_stage_key': 'text', 'conditioning_key': 'crossattn', 'target': 'audioldm2.latent_diffusion.modules.encoders.modules.FlanT5HiddenState'}}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/purnima/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/purnima/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionWrapper has 265.53 M params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/purnima/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model_name = 'audioldm_16k_crossattn_t5'\n",
    "latent_t_per_second=25.6\n",
    "sample_rate=16000\n",
    "duration = 10.0 #Duration is minimum 10 secs. The generated sounds are weird for <10secs\n",
    "guidance_scale = 3\n",
    "random_seed = 42\n",
    "n_candidates = 1\n",
    "batch_size = 1\n",
    "ddim_steps = 20\n",
    "\n",
    "latent_diffusion = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8b357-0962-4d9a-851e-840ee0b3c51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c042ab-f124-410c-93a0-b7bd90a9a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pairs():\n",
    "    adjective_noun_pairs = []\n",
    "    verb_noun_pairs = []\n",
    "\n",
    "    with open('AudioPairBank/adjective_noun_pairs.txt') as f:\n",
    "        adjective_noun_pairs_str = f.read()\n",
    "\n",
    "    with open('AudioPairBank/verb_noun_pairs.txt') as f:\n",
    "        verb_noun_pairs_str = f.read()\n",
    "\n",
    "    adjective_noun_pairs = adjective_noun_pairs_str.split(\"\\n\")\n",
    "    verb_noun_pairs = verb_noun_pairs_str.split(\"\\n\")\n",
    "\n",
    "    return np.array(adjective_noun_pairs), np.array(verb_noun_pairs)\n",
    "\n",
    "adjective_noun_pairs, verb_noun_pairs = get_word_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c3f922-91c7-4f63-8295-1910807788a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 359)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjective_noun_pairs), len(verb_noun_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "959423af-ec21-41af-87e6-ac342a662fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_wav_text(wav_arr, text, random_seed): #return change in score and pearson corr\n",
    "    seed_everything(random_seed)\n",
    "    score_list = []\n",
    "    for ind, wav in enumerate(wavs):\n",
    "        wav = torch.from_numpy(wav).unsqueeze(dim=0).cuda()\n",
    "        score = latent_diffusion.clap.cos_similarity(wav, \"The sound of \"+text.split(\" \")[0])\n",
    "        # print(ind, score)\n",
    "        score_list.append(score.detach().cpu())\n",
    "\n",
    "    change_in_score = torch.abs(score_list[0] - score_list[-1])\n",
    "    corr = np.abs(pearsonr(score_list, np.arange(0, len(score_list), 1)).statistic)\n",
    "\n",
    "    return change_in_score, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861a80d-9421-45d9-927c-f1dab79f4b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d9505d6-706d-46da-9330-aff08c3953ee",
   "metadata": {},
   "source": [
    "# Morphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33e096-4218-40a6-8745-aa6bee866ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beaf56b1-d38d-4320-9dbf-6b582ac011ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstrap = 100\n",
    "num_bootstrap_choices = 50\n",
    "random_seed = 1947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e79cf86c-d497-479b-a35a-886344a26473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(num_choices=1, source_type=\"adjective\", target_type=\"adjective\"):\n",
    "    source_pairs_choices = []\n",
    "    target_pairs_choices = []\n",
    "    dir_p = \"\"\n",
    "    if source_type == 'adjective':\n",
    "        source_pairs_choices = np.random.choice(adjective_noun_pairs, num_choices, replace=False)\n",
    "        if target_type == 'adjective':\n",
    "            target_pairs_choices = np.random.choice(adjective_noun_pairs, num_choices, replace=False)\n",
    "            dir_p = os.path.join('samples_morph', 'adjective_noun_to_adjective_noun')\n",
    "        else:\n",
    "            target_pairs_choices = np.random.choice(verb_noun_pairs, num_choices, replace=False)\n",
    "            dir_p = os.path.join('samples_morph', 'adjective_noun_to_verb_noun')\n",
    "    elif source_type == 'verb':\n",
    "        source_pairs_choices = np.random.choice(verb_noun_pairs, num_choices, replace=False)\n",
    "        if target_type == 'verb':\n",
    "            target_pairs_choices = np.random.choice(verb_noun_pairs, num_choices, replace=False)\n",
    "            dir_p = os.path.join('samples_morph', 'verb_noun_to_verb_noun')\n",
    "        else:\n",
    "            target_pairs_choices = np.random.choice(adjective_noun_pairs, num_choices, replace=False)\n",
    "            dir_p = os.path.join('samples_morph', 'verb_noun_to_adjective_noun')\n",
    "\n",
    "    os.makedirs(dir_p, exist_ok=True)\n",
    "    for source_ind, source_pairs_choice in enumerate(source_pairs_choices):\n",
    "        target_pairs_choice = target_pairs_choices[source_ind]\n",
    "        print(source_pairs_choice, target_pairs_choice)\n",
    "        \n",
    "        for i in np.arange(0,1.1,0.1):\n",
    "            predicted_wav, _ = sample_diffusion_attention_core(source_text=source_pairs_choice, target_text=target_pairs_choice, random_seed=random_seed, ddim_steps=20,\\\n",
    "                                                   latent_diffusion=latent_diffusion,\n",
    "                                                   interpolation_level = i,\n",
    "                                                   interpolate_terms = ['k','v'],\n",
    "                                                   disable_tqdmoutput = True\n",
    "                                                   )\n",
    "           \n",
    "            fname = source_pairs_choice.replace(\" \",\"_\")+\"_to_\"+target_pairs_choice.replace(\" \",\"_\")\n",
    "            fname += \"_\"+'{0:.1f}'.format(i)+\".wav\"\n",
    "            \n",
    "            sf.write(os.path.join(dir_p, fname), predicted_wav, samplerate=16000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e82f6-84ac-456d-8b36-8b0e99b8ed5c",
   "metadata": {},
   "source": [
    "## Adjective Noun to Adjective Noun morphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47284517-52f7-4b5c-9d22-424413d46091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharp metal rattling wind\n",
      "windy wind rattling machine\n",
      "weird engine weird drama\n",
      "distant rain echoing water\n",
      "funny english crazy man\n",
      "extreme noise mechanical noise\n",
      "scary bird heavy man\n",
      "mechanical engine natural forest\n",
      "funny comedy dramatic background\n",
      "weird mood scary man\n",
      "funny gaming evil metal\n",
      "creepy fear evil monster\n",
      "mechanical voice static man\n",
      "weird monster happy child\n",
      "industrial train echoing cell\n",
      "noisy atmosphere scary woman\n",
      "heavy thunder evil ghost\n",
      "weird race funny laughter\n",
      "rumbling metal chaotic noise\n",
      "funny man creepy zombie\n",
      "disturbing horror windy forest\n",
      "evil noise loud computer\n",
      "funny woman strange music\n",
      "happy music creepy thriller\n",
      "calm summer slow phantom\n",
      "heavy death scary metal\n",
      "scary man creepy phantom\n",
      "tropical waterfall industrial noise\n",
      "funny water echoing bathroom\n",
      "funny animal annoying siren\n",
      "crazy alien \n",
      "cute woman heavy wood\n",
      "annoying woman loud military\n",
      "rattling motion tropical waterfall\n",
      "dramatic guitar industrial field\n",
      "mechanical speech echoing hands\n",
      "natural wildlife dramatic space\n",
      "rattling glass creepy mood\n",
      "sharp horror evil horror\n",
      "echoing cell chaotic can\n",
      "peaceful forest flapping wings\n",
      "slow noise quiet background\n",
      "slow piano extreme thunder\n",
      "chaotic garbage tropical insects\n",
      "weird space creepy voice\n",
      "dramatic trailer strange man\n",
      "scary tension loud engine\n",
      "creepy man peaceful park\n",
      "scary terror chaotic cup\n",
      "funny pain annoying air\n",
      "happy crowd extreme car\n",
      "quiet city scary atmosphere\n",
      "tropical forest peaceful morning\n",
      "scary horror creepy house\n",
      "evil words tropical nature\n",
      "echoing cup noisy wind\n",
      "weird ball distant bird\n",
      "industrial power relaxing river\n",
      "windy atmosphere happy effect\n",
      "mechanical robot natural noise\n",
      "rural noise industrial space\n",
      "rattling atmosphere industrial fan\n",
      "natural sounds dramatic guitar\n",
      "mechanical steam scary laughter\n",
      "creepy background evil space\n",
      "evil sign funny scream\n",
      "fast train weird terror\n",
      "mechanical building industrial door\n",
      "creepy monster crazy horror\n",
      "static music echoing dance\n",
      "funny guitar industrial factory\n",
      "peaceful morning evil effect\n",
      "fast sound weird man\n",
      "funny noise dramatic fear\n",
      "loud metal heavy wind\n",
      "rattling engine funny guitar\n",
      "loud noise rattling metal\n",
      "relaxing wind creepy nightmare\n",
      "noisy wind scary effect\n",
      "weird noise rumbling thunder\n",
      "calm evening echoing guitar\n",
      "noisy city weird death\n",
      "creepy halloween weird ball\n",
      "crazy woman scary tension\n",
      "industrial construction scary zombie\n",
      "distant train creepy sound\n",
      "scary water quiet car\n",
      "echoing sound funny fart\n",
      "distant room static english\n",
      "rattling metal evil laughter\n",
      "weird siren scary suspense\n",
      "dramatic cinema echoing terror\n",
      "tropical bird weird space\n",
      "mechanical alien mechanical building\n",
      "creepy insects creepy engine\n",
      "funny horror peaceful woods\n",
      "echoing alien echoing footsteps\n",
      "calm woods evil man\n",
      "relaxing stream rural summer\n",
      "loud wind funny english\n"
     ]
    }
   ],
   "source": [
    "create_samples(num_choices=100, source_type=\"adjective\", target_type=\"adjective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f88485-c92d-4b7a-9764-5edb460fc7f0",
   "metadata": {},
   "source": [
    "## Adjective Noun to Verb Noun morphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8588c7a-cbe8-47fb-9327-75f852308aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static voice crying scream\n",
      "calm waves clapping noise\n",
      "extreme car screaming alien\n",
      "weird woman talking atmosphere\n",
      "noisy wind dripping river\n",
      "static american splashing bubbles\n",
      "loud fight talking lady\n",
      "echoing alien banging thunder\n",
      "weird cell laughing phantom\n",
      "chaotic noise clicking wind\n",
      "echoing phone opening footsteps\n",
      "happy music squeaking water\n",
      "weird fear opening metal\n",
      "weird effect gurgling river\n",
      "rattling machine opening cap\n",
      "loud guitar banging military\n",
      "evil drama growling dragon\n",
      "exotic zoo howling animal\n",
      "scary laughter screaming man\n",
      "fast sound exploding bomb\n",
      "echoing woman falling metal\n",
      "scary zombies crying guitar\n",
      "happy spring breaking ice\n",
      "loud horror dripping metal\n",
      "creepy house squeaking chair\n",
      "extreme house ringing cup\n",
      "sad background ringing noise\n",
      "scary zombie rolling coin\n",
      "calm woods whistling train\n",
      "calm noise screaming baby\n",
      "windy tree screaming woman\n",
      "evil metal opening rain\n",
      "calm wind talking ghost\n",
      "windy rain \n",
      "slow terror laughing voice\n",
      "mechanical factory banging bird\n",
      "chaotic cup crying voice\n",
      "loud train starting machine\n",
      "mechanical operation growling wolf\n",
      "distant thunder banging pot\n",
      "quiet nature crying baby\n",
      "evil terror screaming crowd\n",
      "fast water squeaking toy\n",
      "happy man breathing dog\n",
      "evil words breaking glasses\n",
      "mechanical bell clicking gun\n",
      "evil audio splashing stream\n",
      "relaxing forest breaking window\n",
      "echoing terror screaming animal\n",
      "creepy voice whistling wind\n",
      "strange background crackling snow\n",
      "dramatic tension ringing alert\n",
      "evil monster whistling noise\n",
      "calm nature ringing glasses\n",
      "loud crowd rushing water\n",
      "weird death growling creatures\n",
      "echoing monster accelerating car\n",
      "rural farm screaming child\n",
      "slow horror clicking door\n",
      "distant bird passing noise\n",
      "scary piano squeaking metal\n",
      "natural background crackling wind\n",
      "tropical insects opening can\n",
      "annoying air crackling water\n",
      "weird glitch falling can\n",
      "funny effect splashing fountain\n",
      "weird wind gurgling bubbles\n",
      "epic effect splashing rock\n",
      "mechanical water passing train\n",
      "frightening noise talking kids\n",
      "fast footsteps banging noise\n",
      "slow music screaming english\n",
      "creepy ghost crying child\n",
      "loud effect laughing man\n",
      "frightening terror talking woman\n",
      "calm winter banging fireworks\n",
      "tropical zoo ringing bell\n",
      "creepy space screaming noise\n",
      "funny space gurgling stream\n",
      "slow stop howling monster\n",
      "strange mood clicking wood\n",
      "creepy wind talking voice\n",
      "peaceful park clicking fire\n",
      "crazy effects splashing bird\n",
      "funny suspense howling noise\n",
      "funny words walking footsteps\n",
      "industrial engine laughing kids\n",
      "epic fear flying military\n",
      "rural background groaning voice\n",
      "scary man screaming bird\n",
      "dramatic music falling rock\n",
      "sad melancholy waving water\n",
      "crazy speech squeaking glass\n",
      "industrial robot breaking bones\n",
      "strange alien banging door\n",
      "scary night accelerating race\n",
      "epic music whispering english\n",
      "echoing guitar laughing speech\n",
      "weird drama splashing creek\n",
      "weird ship rolling wood\n"
     ]
    }
   ],
   "source": [
    "create_samples(num_choices=100, source_type=\"adjective\", target_type=\"verb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd224293-23c6-43f6-a699-d4e4e7bf9183",
   "metadata": {},
   "source": [
    "# Verb Noun to Verb Noun morphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "440aee2c-3c38-481e-80eb-21bedc1b9da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening can dripping river\n",
      "breaking wood laughing woman\n",
      "screaming fight singing voice\n",
      "banging pot falling wind\n",
      "talking noise splashing water\n",
      "gurgling stream breathing man\n",
      "falling water screaming voice\n",
      "flying airplane laughing man\n",
      "falling wood clicking glitch\n",
      "splashing rain accelerating car\n",
      "breaking glass whining noise\n",
      "dripping river falling body\n",
      "groaning monster walking feet\n",
      "groaning voice ringing siren\n",
      "ringing glasses banging war\n",
      "groaning zombie breaking glass\n",
      "howling wind talking kids\n",
      "whistling bird singing woman\n",
      "banging car gurgling toilet\n",
      "rushing water waving lake\n",
      "falling noise falling rock\n",
      "talking crowd howling wind\n",
      "flying helicopter talking woman\n",
      "screaming woman whistling air\n",
      "opening glass growling zombie\n",
      "banging fireworks breaking box\n",
      "splashing glass crying dog\n",
      "splashing water crying voice\n",
      "squeaking wind singing choir\n",
      "growling creatures crackling bones\n",
      "falling body crying man\n",
      "breaking noise screaming baby\n",
      "squeaking feet talking baby\n",
      "growling ghost singing church_bell\n",
      "growling guitar breaking ice\n",
      "clicking machine talking alien\n",
      "squeaking box opening bell\n",
      "squeaking wood screaming bird\n",
      "whistling engine dripping water\n",
      "screaming animal banging army\n",
      "crying monster falling can\n",
      "clicking toy flying engine\n",
      "crackling ice opening box\n",
      "crying man opening rain\n",
      "opening voice talking ghost\n",
      "singing woman whispering speech\n",
      "crying guitar squeaking metal\n",
      "splashing stream whistling engine\n",
      "falling glass splashing rain\n",
      "dripping rain splashing creek\n",
      "honking car ringing noise\n",
      "banging thunder screaming ghost\n",
      "talking man passing train\n",
      "opening coke rolling can\n",
      "growling noise walking shoes\n",
      "splashing bird opening laughter\n",
      "passing airplane laughing baby\n",
      "walking woman screaming monster\n",
      "opening beer opening window\n",
      "crackling water opening glass\n",
      "talking monster laughing english\n",
      "ringing metal banging thunder\n",
      "waving river growling dragon\n",
      "talking child banging metal\n",
      "whispering monster crying baby\n",
      "breaking footsteps splashing bath\n",
      "banging wood banging gun\n",
      "falling bird talking computer\n",
      "squeaking house singing mass\n",
      "dripping stream opening trash\n",
      "whispering alien clicking noise\n",
      "talking atmosphere ringing glasses\n",
      "flying engine squeaking feet\n",
      "talking kids rushing water\n",
      "laughing baby whistling train\n",
      "whistling wind laughing crowd\n",
      "breathing woman squeaking chair\n",
      "growling bird opening helicopter\n",
      "starting machine growling pet\n",
      "howling monster breathing monster\n",
      "clicking office splashing ship\n",
      "opening box crying animal\n",
      "accelerating race opening cap\n",
      "crying ghost banging car\n",
      "growling monster clicking door\n",
      "opening book splashing lake\n",
      "whistling train opening ship\n",
      "ticking bell opening voice\n",
      "gurgling creek flying fly\n",
      "breaking glasses screaming child\n",
      "breaking waves screaming zombie\n",
      "whispering speech falling noise\n",
      "walking heels banging military\n",
      "whistling steam \n",
      "howling dog tweeting bird\n",
      "gurgling toilet breaking cup\n",
      "singing voice splashing stream\n",
      "opening water burning wood\n",
      "opening thunder squeaking footsteps\n",
      "flying fly splashing rock\n"
     ]
    }
   ],
   "source": [
    "create_samples(num_choices=100, source_type=\"verb\", target_type=\"verb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914ebb1-96df-41c6-92fd-584044f600ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91c56995-0df4-473f-8d44-2b8698b02f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'annoying_woman_to_loud_military'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "babb7319-e955-4755-8fa9-ef30d7474ad9",
   "metadata": {},
   "source": [
    "# Evaluate Adjective Nouns to Adjective Nouns morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92788f34-d839-4be6-b5f8-5614478801ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annoying_woman loud_military\n",
      "calm_evening echoing_guitar\n",
      "calm_summer slow_phantom\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     audio, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(fname, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m     31\u001b[0m     wavs\u001b[38;5;241m.\u001b[39mappend(audio)\n\u001b[0;32m---> 32\u001b[0m change_in_score, perceptual_lin \u001b[38;5;241m=\u001b[39m \u001b[43mscore_wav_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_noun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m adj_noun_to_adj_noun_clap_score_change_list\u001b[38;5;241m.\u001b[39mappend(change_in_score)\n\u001b[1;32m     34\u001b[0m adj_noun_to_adj_noun_perceptual_lin_score_list\u001b[38;5;241m.\u001b[39mappend(perceptual_lin)\n",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m, in \u001b[0;36mscore_wav_text\u001b[0;34m(wav_arr, text, random_seed)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, wav \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(wavs):\n\u001b[1;32m      5\u001b[0m     wav \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(wav)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 6\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_diffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe sound of \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(ind, score)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     score_list\u001b[38;5;241m.\u001b[39mappend(score\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m/media/appdir2/Github/audioldm2/eval/../audioldm2/latent_diffusion/modules/encoders/modules.py:694\u001b[0m, in \u001b[0;36mCLAPAudioEmbeddingClassifierFreev2.cos_similarity\u001b[0;34m(self, waveform, text)\u001b[0m\n\u001b[1;32m    692\u001b[0m         audio_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(waveform\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 694\u001b[0m     text_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(audio_emb, text_emb, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_mode \u001b[38;5;241m=\u001b[39m original_embed_mode\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/appdir2/Github/audioldm2/eval/../audioldm2/latent_diffusion/modules/encoders/modules.py:772\u001b[0m, in \u001b[0;36mCLAPAudioEmbeddingClassifierFreev2.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m text_data\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    770\u001b[0m                 text_data[key] \u001b[38;5;241m=\u001b[39m text_data[key]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 772\u001b[0m         embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m embed \u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(embed\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n",
      "File \u001b[0;32m/media/appdir2/Github/audioldm2/eval/../audioldm2/clap/open_clip/model.py:747\u001b[0m, in \u001b[0;36mCLAP.get_text_embedding\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m    746\u001b[0m     data[k] \u001b[38;5;241m=\u001b[39m data[k]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 747\u001b[0m text_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m text_embeds \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(text_embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_embeds\n",
      "File \u001b[0;32m/media/appdir2/Github/audioldm2/eval/../audioldm2/clap/open_clip/model.py:657\u001b[0m, in \u001b[0;36mCLAP.encode_text\u001b[0;34m(self, text, device)\u001b[0m\n\u001b[1;32m    655\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection(x)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_branch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 657\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_branch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooler_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    663\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection(x)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_branch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbart\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    450\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    451\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 453\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/transformers/pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:466\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    465\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 466\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:378\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    377\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 378\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "File \u001b[0;32m~/anaconda3/envs/audioldm2/lib/python3.8/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir_p = os.path.join('samples_morph', 'adjective_noun_to_adjective_noun')\n",
    "\n",
    "adjective_noun_to_adjective_noun_list = []\n",
    "for f in os.listdir(dir_p):\n",
    "    tokens = \"_\".join(f.split(\".wav\")[0].split(\"_\")[:-1])\n",
    "    adjective_noun_to_adjective_noun_list.append(tokens)\n",
    "adjective_noun_to_adjective_noun_list = np.unique(np.array(adjective_noun_to_adjective_noun_list))\n",
    "\n",
    "adj_noun_list = []\n",
    "verb_noun_list = []\n",
    "for f in adjective_noun_to_adjective_noun_list:\n",
    "    tokens = f.split(\".\")[0].split(\"_to_\")\n",
    "    adj_noun_list.append(tokens[0])\n",
    "    verb_noun_list.append(tokens[1])\n",
    "\n",
    "\n",
    "adj_noun_to_adj_noun_clap_score_change_list = []\n",
    "adj_noun_to_adj_noun_perceptual_lin_score_list = []\n",
    "\n",
    "\n",
    "for adj_ind, adj_noun in enumerate(adj_noun_list):\n",
    "    verb_noun = verb_noun_list[adj_ind]\n",
    "\n",
    "    print(adj_noun, verb_noun)\n",
    "    \n",
    "    wavs = []\n",
    "    for i in np.arange(0,1.1,0.1):\n",
    "        i_str = '{0:.1f}'.format(i)\n",
    "        fname = os.path.join(dir_p, adj_noun.replace(\" \",\"_\")+\"_to_\"+verb_noun.replace(\" \",\"_\")+\"_\"+i_str+\".wav\")\n",
    "        audio, _ = librosa.load(fname, sr=16000)\n",
    "        wavs.append(audio)\n",
    "    change_in_score, perceptual_lin = score_wav_text(wavs, adj_noun, random_seed=random_seed)\n",
    "    adj_noun_to_adj_noun_clap_score_change_list.append(change_in_score)\n",
    "    adj_noun_to_adj_noun_perceptual_lin_score_list.append(perceptual_lin)\n",
    "\n",
    "    change_in_score, perceptual_lin = score_wav_text(wavs, verb_noun, random_seed=random_seed)\n",
    "    adj_noun_to_adj_noun_clap_score_change_list.append(change_in_score)\n",
    "    adj_noun_to_adj_noun_perceptual_lin_score_list.append(perceptual_lin)\n",
    "\n",
    "\n",
    "adj_noun_to_adj_noun_clap_score_change_list = torch.stack(adj_noun_to_adj_noun_clap_score_change_list)\n",
    "adj_noun_to_adj_noun_perceptual_lin_score_list = np.array(adj_noun_to_adj_noun_perceptual_lin_score_list)\n",
    "\n",
    "print(\"Mean clap score change = \", str(torch.mean(adj_noun_to_adj_noun_clap_score_change_list).numpy())+\"+/-\"+str(torch.std(adj_noun_to_adj_noun_clap_score_change_list).numpy()))\n",
    "print(\"Mean Corr = \", str(np.mean(adj_noun_to_adj_noun_perceptual_lin_score_list))+\"+/-\"+str(np.std(adj_noun_to_adj_noun_perceptual_lin_score_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee55f4-f94a-40d5-acd7-617edf0414bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b242fcd-57db-4749-a0c5-7448c584e11f",
   "metadata": {},
   "source": [
    "# Evaluate Verb Noun to Verb Noun Morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db763376-222b-452b-9a50-a29d42319c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerating_race opening_cap\n",
      "banging_car gurgling_toilet\n",
      "banging_fireworks breaking_box\n",
      "banging_pot falling_wind\n",
      "banging_thunder screaming_ghost\n",
      "banging_wood banging_gun\n",
      "breaking_footsteps splashing_bath\n",
      "breaking_glass whining_noise\n",
      "breaking_glasses screaming_child\n",
      "breaking_noise screaming_baby\n",
      "breaking_waves screaming_zombie\n",
      "breaking_wood laughing_woman\n",
      "breathing_woman squeaking_chair\n",
      "clicking_machine talking_alien\n",
      "clicking_office splashing_ship\n",
      "clicking_toy flying_engine\n",
      "crackling_ice opening_box\n",
      "crackling_water opening_glass\n",
      "crying_ghost banging_car\n",
      "crying_guitar squeaking_metal\n",
      "crying_man opening_rain\n",
      "crying_monster falling_can\n",
      "dripping_rain splashing_creek\n",
      "dripping_river falling_body\n",
      "dripping_stream opening_trash\n",
      "falling_bird talking_computer\n",
      "falling_body crying_man\n",
      "falling_glass splashing_rain\n",
      "falling_noise falling_rock\n",
      "falling_water screaming_voice\n",
      "falling_wood clicking_glitch\n",
      "flying_airplane laughing_man\n",
      "flying_engine squeaking_feet\n",
      "flying_fly splashing_rock\n",
      "flying_helicopter talking_woman\n",
      "groaning_monster walking_feet\n",
      "groaning_voice ringing_siren\n",
      "groaning_zombie breaking_glass\n",
      "growling_bird opening_helicopter\n",
      "growling_creatures crackling_bones\n",
      "growling_ghost singing_church_bell\n",
      "growling_guitar breaking_ice\n",
      "growling_monster clicking_door\n",
      "growling_noise walking_shoes\n",
      "gurgling_creek flying_fly\n",
      "gurgling_stream breathing_man\n",
      "gurgling_toilet breaking_cup\n",
      "honking_car ringing_noise\n",
      "howling_dog tweeting_bird\n",
      "howling_monster breathing_monster\n",
      "howling_wind talking_kids\n",
      "laughing_baby whistling_train\n",
      "opening_beer opening_window\n",
      "opening_book splashing_lake\n",
      "opening_box crying_animal\n",
      "opening_can dripping_river\n",
      "opening_coke rolling_can\n",
      "opening_glass growling_zombie\n",
      "opening_thunder squeaking_footsteps\n",
      "opening_voice talking_ghost\n",
      "opening_water burning_wood\n",
      "passing_airplane laughing_baby\n",
      "ringing_glasses banging_war\n",
      "ringing_metal banging_thunder\n",
      "rushing_water waving_lake\n",
      "screaming_animal banging_army\n",
      "screaming_fight singing_voice\n",
      "screaming_woman whistling_air\n",
      "singing_voice splashing_stream\n",
      "singing_woman whispering_speech\n",
      "splashing_bird opening_laughter\n",
      "splashing_glass crying_dog\n",
      "splashing_rain accelerating_car\n",
      "splashing_stream whistling_engine\n",
      "splashing_water crying_voice\n",
      "squeaking_box opening_bell\n",
      "squeaking_feet talking_baby\n",
      "squeaking_house singing_mass\n",
      "squeaking_wind singing_choir\n",
      "squeaking_wood screaming_bird\n",
      "starting_machine growling_pet\n",
      "talking_atmosphere ringing_glasses\n",
      "talking_child banging_metal\n",
      "talking_crowd howling_wind\n",
      "talking_kids rushing_water\n",
      "talking_man passing_train\n",
      "talking_monster laughing_english\n",
      "talking_noise splashing_water\n",
      "ticking_bell opening_voice\n",
      "walking_heels banging_military\n",
      "walking_woman screaming_monster\n",
      "waving_river growling_dragon\n",
      "whispering_alien clicking_noise\n",
      "whispering_monster crying_baby\n",
      "whispering_speech falling_noise\n",
      "whistling_bird singing_woman\n",
      "whistling_engine dripping_water\n",
      "whistling_steam \n",
      "whistling_train opening_ship\n",
      "whistling_wind laughing_crowd\n",
      "Mean clap score change =  0.14810182+/-0.091420494\n",
      "Mean Corr =  0.4954706742606861+/-0.2617641851004409\n"
     ]
    }
   ],
   "source": [
    "dir_p = os.path.join('samples_morph', 'verb_noun_to_verb_noun')\n",
    "\n",
    "adjective_noun_to_adjective_noun_list = []\n",
    "for f in os.listdir(dir_p):\n",
    "    tokens = \"_\".join(f.split(\".wav\")[0].split(\"_\")[:-1])\n",
    "    adjective_noun_to_adjective_noun_list.append(tokens)\n",
    "adjective_noun_to_adjective_noun_list = np.unique(np.array(adjective_noun_to_adjective_noun_list))\n",
    "\n",
    "adj_noun_list = []\n",
    "verb_noun_list = []\n",
    "for f in adjective_noun_to_adjective_noun_list:\n",
    "    tokens = f.split(\".\")[0].split(\"_to_\")\n",
    "    adj_noun_list.append(tokens[0])\n",
    "    verb_noun_list.append(tokens[1])\n",
    "\n",
    "\n",
    "verb_noun_to_verb_noun_clap_score_change_list = []\n",
    "verb_noun_to_verb_noun_perceptual_lin_score_list = []\n",
    "\n",
    "\n",
    "for adj_ind, adj_noun in enumerate(adj_noun_list):\n",
    "    verb_noun = verb_noun_list[adj_ind]\n",
    "\n",
    "    print(adj_noun, verb_noun)\n",
    "    \n",
    "    wavs = []\n",
    "    for i in np.arange(0,1.1,0.1):\n",
    "        i_str = '{0:.1f}'.format(i)\n",
    "        fname = os.path.join(dir_p, adj_noun.replace(\" \",\"_\")+\"_to_\"+verb_noun.replace(\" \",\"_\")+\"_\"+i_str+\".wav\")\n",
    "        audio, _ = librosa.load(fname, sr=16000)\n",
    "        wavs.append(audio)\n",
    "    change_in_score, perceptual_lin = score_wav_text(wavs, adj_noun, random_seed=random_seed)\n",
    "    verb_noun_to_verb_noun_clap_score_change_list.append(change_in_score)\n",
    "    verb_noun_to_verb_noun_perceptual_lin_score_list.append(perceptual_lin)\n",
    "\n",
    "    change_in_score, perceptual_lin = score_wav_text(wavs, verb_noun, random_seed=random_seed)\n",
    "    verb_noun_to_verb_noun_clap_score_change_list.append(change_in_score)\n",
    "    verb_noun_to_verb_noun_perceptual_lin_score_list.append(perceptual_lin)\n",
    "\n",
    "\n",
    "verb_noun_to_verb_noun_clap_score_change_list = torch.stack(verb_noun_to_verb_noun_clap_score_change_list)\n",
    "verb_noun_to_verb_noun_perceptual_lin_score_list = np.array(verb_noun_to_verb_noun_perceptual_lin_score_list)\n",
    "\n",
    "print(\"Mean clap score change = \", str(torch.mean(verb_noun_to_verb_noun_clap_score_change_list).numpy())+\"+/-\"+str(torch.std(verb_noun_to_verb_noun_clap_score_change_list).numpy()))\n",
    "print(\"Mean Corr = \", str(np.mean(verb_noun_to_verb_noun_perceptual_lin_score_list))+\"+/-\"+str(np.std(verb_noun_to_verb_noun_perceptual_lin_score_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16204e7a-f9cb-433d-a377-aa5852f61088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "236003ef-0c20-4bbd-8f6b-84c0e92e3230",
   "metadata": {},
   "source": [
    "# Evaluate Adjective Noun to Verb Noun Morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3292df8-ae03-49fe-825f-2ab2fd80255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annoying_air crackling_water\n",
      "calm_nature ringing_glasses\n",
      "calm_noise screaming_baby\n",
      "calm_waves clapping_noise\n",
      "calm_wind talking_ghost\n",
      "calm_winter banging_fireworks\n",
      "calm_woods whistling_train\n",
      "chaotic_cup crying_voice\n",
      "chaotic_noise clicking_wind\n",
      "crazy_effects splashing_bird\n",
      "crazy_speech squeaking_glass\n",
      "creepy_ghost crying_child\n",
      "creepy_house squeaking_chair\n",
      "creepy_space screaming_noise\n",
      "creepy_voice whistling_wind\n",
      "creepy_wind talking_voice\n",
      "distant_bird passing_noise\n",
      "distant_thunder banging_pot\n",
      "dramatic_music falling_rock\n",
      "dramatic_tension ringing_alert\n",
      "echoing_alien banging_thunder\n",
      "echoing_guitar laughing_speech\n",
      "echoing_monster accelerating_car\n",
      "echoing_phone opening_footsteps\n",
      "echoing_terror screaming_animal\n",
      "echoing_woman falling_metal\n",
      "epic_effect splashing_rock\n",
      "epic_fear flying_military\n",
      "epic_music whispering_english\n",
      "evil_audio splashing_stream\n",
      "evil_drama growling_dragon\n",
      "evil_metal opening_rain\n",
      "evil_monster whistling_noise\n",
      "evil_terror screaming_crowd\n",
      "evil_words breaking_glasses\n",
      "exotic_zoo howling_animal\n",
      "extreme_car screaming_alien\n",
      "extreme_house ringing_cup\n",
      "fast_footsteps banging_noise\n",
      "fast_sound exploding_bomb\n",
      "fast_water squeaking_toy\n",
      "frightening_noise talking_kids\n",
      "frightening_terror talking_woman\n",
      "funny_effect splashing_fountain\n",
      "funny_space gurgling_stream\n",
      "funny_suspense howling_noise\n",
      "funny_words walking_footsteps\n",
      "happy_man breathing_dog\n",
      "happy_music squeaking_water\n",
      "happy_spring breaking_ice\n",
      "industrial_engine laughing_kids\n",
      "industrial_robot breaking_bones\n",
      "loud_crowd rushing_water\n",
      "loud_effect laughing_man\n",
      "loud_fight talking_lady\n",
      "loud_guitar banging_military\n",
      "loud_horror dripping_metal\n",
      "loud_train starting_machine\n",
      "mechanical_bell clicking_gun\n",
      "mechanical_factory banging_bird\n",
      "mechanical_operation growling_wolf\n",
      "mechanical_water passing_train\n",
      "natural_background crackling_wind\n",
      "noisy_wind dripping_river\n",
      "peaceful_park clicking_fire\n",
      "quiet_nature crying_baby\n",
      "rattling_machine opening_cap\n",
      "relaxing_forest breaking_window\n",
      "rural_background groaning_voice\n",
      "rural_farm screaming_child\n",
      "sad_background ringing_noise\n",
      "sad_melancholy waving_water\n",
      "scary_laughter screaming_man\n",
      "scary_man screaming_bird\n",
      "scary_night accelerating_race\n",
      "scary_piano squeaking_metal\n",
      "scary_zombie rolling_coin\n",
      "scary_zombies crying_guitar\n",
      "slow_horror clicking_door\n",
      "slow_music screaming_english\n",
      "slow_stop howling_monster\n",
      "slow_terror laughing_voice\n",
      "static_american splashing_bubbles\n",
      "static_voice crying_scream\n",
      "strange_alien banging_door\n",
      "strange_background crackling_snow\n",
      "strange_mood clicking_wood\n",
      "tropical_insects opening_can\n",
      "tropical_zoo ringing_bell\n",
      "weird_cell laughing_phantom\n",
      "weird_death growling_creatures\n",
      "weird_drama splashing_creek\n",
      "weird_effect gurgling_river\n",
      "weird_fear opening_metal\n",
      "weird_glitch falling_can\n",
      "weird_ship rolling_wood\n",
      "weird_wind gurgling_bubbles\n",
      "weird_woman talking_atmosphere\n",
      "windy_rain \n",
      "windy_tree screaming_woman\n",
      "Mean clap score change =  0.12727438+/-0.0773382\n",
      "Mean Corr =  0.4739036863798512+/-0.2704492643031836\n"
     ]
    }
   ],
   "source": [
    "dir_p = os.path.join('samples_morph', 'adjective_noun_to_verb_noun')\n",
    "\n",
    "adjective_noun_to_adjective_noun_list = []\n",
    "for f in os.listdir(dir_p):\n",
    "    tokens = \"_\".join(f.split(\".wav\")[0].split(\"_\")[:-1])\n",
    "    adjective_noun_to_adjective_noun_list.append(tokens)\n",
    "adjective_noun_to_adjective_noun_list = np.unique(np.array(adjective_noun_to_adjective_noun_list))\n",
    "\n",
    "adj_noun_list = []\n",
    "verb_noun_list = []\n",
    "for f in adjective_noun_to_adjective_noun_list:\n",
    "    tokens = f.split(\".\")[0].split(\"_to_\")\n",
    "    adj_noun_list.append(tokens[0])\n",
    "    verb_noun_list.append(tokens[1])\n",
    "\n",
    "\n",
    "adjective_noun_to_verb_noun_clap_score_change_list = []\n",
    "adjective_noun_to_verb_noun_perceptual_lin_score_list = []\n",
    "\n",
    "\n",
    "for adj_ind, adj_noun in enumerate(adj_noun_list):\n",
    "    verb_noun = verb_noun_list[adj_ind]\n",
    "\n",
    "    print(adj_noun, verb_noun)\n",
    "    \n",
    "    wavs = []\n",
    "    for i in np.arange(0,1.1,0.1):\n",
    "        i_str = '{0:.1f}'.format(i)\n",
    "        fname = os.path.join(dir_p, adj_noun.replace(\" \",\"_\")+\"_to_\"+verb_noun.replace(\" \",\"_\")+\"_\"+i_str+\".wav\")\n",
    "        audio, _ = librosa.load(fname, sr=16000)\n",
    "        wavs.append(audio)\n",
    "    change_in_score, perceptual_lin = score_wav_text(wavs, adj_noun, random_seed=random_seed)\n",
    "    adjective_noun_to_verb_noun_clap_score_change_list.append(change_in_score)\n",
    "    adjective_noun_to_verb_noun_perceptual_lin_score_list.append(perceptual_lin)\n",
    "\n",
    "    change_in_score, perceptual_lin = score_wav_text(wavs, verb_noun, random_seed=random_seed)\n",
    "    adjective_noun_to_verb_noun_clap_score_change_list.append(change_in_score)\n",
    "    adjective_noun_to_verb_noun_perceptual_lin_score_list.append(perceptual_lin)\n",
    "\n",
    "\n",
    "adjective_noun_to_verb_noun_clap_score_change_list = torch.stack(adjective_noun_to_verb_noun_clap_score_change_list)\n",
    "adjective_noun_to_verb_noun_perceptual_lin_score_list = np.array(adjective_noun_to_verb_noun_perceptual_lin_score_list)\n",
    "\n",
    "print(\"Mean clap score change = \", str(torch.mean(adjective_noun_to_verb_noun_clap_score_change_list).numpy())+\"+/-\"+str(torch.std(adjective_noun_to_verb_noun_clap_score_change_list).numpy()))\n",
    "print(\"Mean Corr = \", str(np.mean(adjective_noun_to_verb_noun_perceptual_lin_score_list))+\"+/-\"+str(np.std(adjective_noun_to_verb_noun_perceptual_lin_score_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec41d55-b5ea-4647-801f-745019ad3384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993933b8-7d2c-4212-bb37-4a12d320b739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee86ae8-3b3a-4679-8592-3f3f61388ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d042ae-39de-4be9-bda4-b772312a585a",
   "metadata": {},
   "source": [
    "# Bootstrapped Adjective Noun to Adjective Noun Morph scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f86be2-b50f-422a-8c79-257f3faacfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_arr = len(adj_noun_to_adj_noun_clap_score_change_list)\n",
    "\n",
    "bootstrap_clap_score_change_list = []\n",
    "bootstrap_perceptual_lin_score_list = []\n",
    "for bootstrap_ind in range(num_bootstrap):\n",
    "    ind_choices = np.random.choice(np.arange(len_arr), num_bootstrap_choices, replace=False)\n",
    "    score_change_sublist = adj_noun_to_adj_noun_clap_score_change_list[ind_choices]\n",
    "    perceptual_lin_score_sublist = adj_noun_to_adj_noun_perceptual_lin_score_list[ind_choices]\n",
    "\n",
    "    bootstrap_clap_score_change_list.append(torch.mean(score_change_sublist))\n",
    "    bootstrap_perceptual_lin_score_list.append(np.mean(perceptual_lin_score_sublist))\n",
    "\n",
    "bootstrap_clap_score_change_list = torch.stack(bootstrap_clap_score_change_list)\n",
    "bootstrap_perceptual_lin_score_list = np.array(bootstrap_perceptual_lin_score_list)\n",
    "print(torch.mean(bootstrap_clap_score_change_list), np.mean(bootstrap_perceptual_lin_score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc699309-5d01-4242-bfad-0496818ecd90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7bec567-0cd8-4a4e-b193-828d008742e5",
   "metadata": {},
   "source": [
    "# Bootstrapped Verb Noun to Verb Noun morph scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfce310-7796-49e3-881c-0aad5125b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_arr = len(verb_noun_to_verb_noun_clap_score_change_list)\n",
    "\n",
    "bootstrap_clap_score_change_list = []\n",
    "bootstrap_perceptual_lin_score_list = []\n",
    "for bootstrap_ind in range(num_bootstrap):\n",
    "    ind_choices = np.random.choice(np.arange(len_arr), num_bootstrap_choices, replace=False)\n",
    "    score_change_sublist = verb_noun_to_verb_noun_clap_score_change_list[ind_choices]\n",
    "    perceptual_lin_score_sublist = verb_noun_to_verb_noun_perceptual_lin_score_list[ind_choices]\n",
    "\n",
    "    bootstrap_clap_score_change_list.append(torch.mean(score_change_sublist))\n",
    "    bootstrap_perceptual_lin_score_list.append(np.mean(perceptual_lin_score_sublist))\n",
    "\n",
    "bootstrap_clap_score_change_list = torch.stack(bootstrap_clap_score_change_list)\n",
    "bootstrap_perceptual_lin_score_list = np.array(bootstrap_perceptual_lin_score_list)\n",
    "print(torch.mean(bootstrap_clap_score_change_list), np.mean(bootstrap_perceptual_lin_score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436dcd6-840f-49c3-8da8-ae4dd2a489be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87cd8965-6843-42e2-9676-6b9ac8b4bf6e",
   "metadata": {},
   "source": [
    "# Bootstrapped Adjective Noun to Verb Noun morph scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987ebf3-361d-4ea1-ba90-3fcbc86d70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_arr = len(adjective_noun_to_verb_noun_clap_score_change_list)\n",
    "\n",
    "bootstrap_clap_score_change_list = []\n",
    "bootstrap_perceptual_lin_score_list = []\n",
    "for bootstrap_ind in range(num_bootstrap):\n",
    "    ind_choices = np.random.choice(np.arange(len_arr), num_bootstrap_choices, replace=False)\n",
    "    score_change_sublist = adj_noun_to_verb_noun_clap_score_change_list[ind_choices]\n",
    "    perceptual_lin_score_sublist = adj_noun_to_verb_noun_perceptual_lin_score_list[ind_choices]\n",
    "\n",
    "    bootstrap_clap_score_change_list.append(torch.mean(score_change_sublist))\n",
    "    bootstrap_perceptual_lin_score_list.append(np.mean(perceptual_lin_score_sublist))\n",
    "\n",
    "bootstrap_clap_score_change_list = torch.stack(bootstrap_clap_score_change_list)\n",
    "bootstrap_perceptual_lin_score_list = np.array(bootstrap_perceptual_lin_score_list)\n",
    "print(torch.mean(bootstrap_clap_score_change_list), np.mean(bootstrap_perceptual_lin_score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5dd331-25a1-449a-91bf-2063251e0599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be694e9-bafa-4aee-a033-b7471c84cfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioldm2",
   "language": "python",
   "name": "audioldm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
